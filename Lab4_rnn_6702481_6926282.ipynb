{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Lab4_rnn_6702481_6926282.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OOCfOBWsc-H9",
        "O40_iF2tc-IS"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PE01RYKc-HS",
        "colab_type": "text"
      },
      "source": [
        "# Lab 4: Recurrent models\n",
        "\n",
        "This lab is supposed to give you some initial practice with neural models in NLP.\n",
        "\n",
        "**This is the complete Lab 4, in two parts.** The purpose of the first part of the lab is to get you started with using neural models. The second part of the lab contains exercises on ELMo embeddings, applying them to the task of word sense disambuiguation following the approach from the original paper by Peters et al.\n",
        "\n",
        "\n",
        "## Part 1 (50 points)\n",
        "\n",
        "In the first part of lab 4, we will play with training a recurrent model for part of speech tagging. As an easy exercise, you will observe what happens when you plug in pretrained word embeddings into an neural NLP model and will experiment with different sizes of training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuUE7sXzc-HT",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1: prepare the data (5 points)\n",
        "\n",
        "Linguistic data come in a variety of formats. You already had a chance to play with POS-annotated corpus data in Lab 1.\n",
        "\n",
        "In the first exercise, you will access POS-annotated data in one format (NLTK) and save it on the disk in a text format. Start with the tagged sentences from the Brown corpus, which can be retrieved as below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVtIzjKLc-HU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "import nltk\n",
        "nltk.download('semcor')\n",
        "from nltk.corpus import brown\n",
        "brown.tagged_sents()\n",
        "!pip install allennlp==0.9\n",
        "import allennlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQqtT8aAc-Ha",
        "colab_type": "text"
      },
      "source": [
        "Now randomize the order of all sentences in the corpus using <code>random.shuffle()</code> function and split it into 50K sentences for training, 5K for validation, and the rest for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diXmnbgYc-Ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Write your code here\n",
        "\n",
        "tagged_sentence = list(brown.tagged_sents())\n",
        "random.shuffle(tagged_sentence)\n",
        "\n",
        "training_brown= tagged_sentence[:50000]\n",
        "validation_brown=tagged_sentence[50000:55000]\n",
        "testing_brown=tagged_sentence[55000:]\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf_n-SKHc-Hd",
        "colab_type": "text"
      },
      "source": [
        "Define a function for saving your datasets to a text file in the following format:\n",
        "* one sentence per line\n",
        "* tokens separated by spaces\n",
        "* POS tag separated from the token by \"###\", for example <code>said###VBD</code>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7UdbcKZc-He",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_posdata(sentences,outfile):\n",
        "    #Write your code here\n",
        "    with open(outfile, 'w') as f:\n",
        "      for list_ in sentences:\n",
        "        for sentence in list_:\n",
        "          temp = '###'.join(sentence[:])\n",
        "          f.write(temp + ' ')\n",
        "        f.write('\\n')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7VluEdsc-Hh",
        "colab_type": "text"
      },
      "source": [
        "Now save your data partitions in different sizes. We will start with small data samples since training on a large dataset may be very slow depending on your machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoW4FHKRc-Hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_posdata(training_brown,\"train_brown.txt\")\n",
        "write_posdata(testing_brown,\"test_brown.txt\")\n",
        "write_posdata(validation_brown,\"validation_brown.txt\")\n",
        "write_posdata(training_brown[:50],\"train_brown_50.txt\")\n",
        "write_posdata(validation_brown[:50],\"validation_brown_50.txt\")\n",
        "write_posdata(training_brown[:500],\"train_brown_500.txt\")\n",
        "write_posdata(validation_brown[:500],\"validation_brown_500.txt\")\n",
        "write_posdata(training_brown[:5000],\"train_brown_5000.txt\")\n",
        "write_posdata(training_brown[:5000],\"validation_brown_5000.txt\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8eO6uFGc-Hk",
        "colab_type": "text"
      },
      "source": [
        "Congratulations, you have now saved the POS tagged data for model training purposes!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmAOuE6tc-Hl",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2: train neural POS tagger models (35 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jcac5JyYc-Hl",
        "colab_type": "text"
      },
      "source": [
        "We will now play with a neural model. First of all, install <code>allennlp</code>. The LSTM model we will train follows the AllenNLP tutorial https://allennlp.org/tutorials which contains ample explanations of the underlying code. Let us start by loading the model code and data, starting with a tiny sample for demonstration purposes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m93y5PIkc-Hm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2e99ea2a-7798-4a23-b10f-5b3bf2461113"
      },
      "source": [
        "from lstm_tutorial import *\n",
        "\n",
        "train_dataset_tiny = reader.read(\"train_brown_50.txt\")\n",
        "validation_dataset_tiny = reader.read(\"validation_brown_50.txt\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50it [00:00, 18449.48it/s]\n",
            "50it [00:00, 9887.56it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5SRi9Xwc-Ho",
        "colab_type": "text"
      },
      "source": [
        "Fist of all we need to initialize the vocabulary and define an embedding (vector) for each token. We set the embedding size at 300, common in realistic applications. By default, the embeddings are initialized randomly and updated during trining (this can be changed but we start with a standard configuration). We also need to specify the <code>HIDDEN_DIM</code> parameter: the dimensionality of the hidden vector representations in the LSTM cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iBOhKmQc-Hp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "478d0476-34b7-4c1d-cc5c-030742f378e0"
      },
      "source": [
        "vocab_tiny = Vocabulary.from_instances(train_dataset_tiny + validation_dataset_tiny)\n",
        "\n",
        "EMBEDDING_DIM = 300\n",
        "HIDDEN_DIM = 20\n",
        "\n",
        "token_embedding_tiny = Embedding(num_embeddings=vocab_tiny.get_vocab_size('tokens'),\n",
        "                            embedding_dim=EMBEDDING_DIM)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 18257.54it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXELUz83c-Hs",
        "colab_type": "text"
      },
      "source": [
        "Download the smallest pretrained word vector model from https://nlp.stanford.edu/projects/glove/, unzip it, and extract the relevant file <code>'glove.6B.300d.txt'</code> in your working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi_dNMgCc-Hs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b50314b0-f758-4650-bf45-0b21def1c8c5"
      },
      "source": [
        "glove_token_embedding_tiny = Embedding.from_params(vocab=vocab_tiny,\n",
        "                            params=Params({'pretrained_file':'glove.6B.300d.txt',\n",
        "                                           'embedding_dim' : EMBEDDING_DIM}))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000it [00:01, 260294.89it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVkibWaLc-Hv",
        "colab_type": "text"
      },
      "source": [
        "Now from embedding a single word with <code>token_embedding_tiny</code> we can proceed to mapping a word sequence into a sequence of vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ah8JHXimc-Hv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_embeddings_tiny = BasicTextFieldEmbedder({\"tokens\": token_embedding_tiny})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYDuPl9mc-Hy",
        "colab_type": "text"
      },
      "source": [
        "The following initializes parameters of an LSTM model using <code>word_embeddings_tiny</code> input encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi7yD59lc-Hy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
        "\n",
        "model_tiny = LstmTagger(word_embeddings_tiny, lstm, vocab_tiny)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNbVqkOLc-H0",
        "colab_type": "text"
      },
      "source": [
        "Now define an LSTM model called <code>glove_model_tiny</code> that uses <code>glove_token_embedding_tiny</code>:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCTdLZ_Dc-H1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#write your code here\n",
        "glove_embeddings_tiny = BasicTextFieldEmbedder({\"tokens\": glove_token_embedding_tiny})\n",
        "glove_model_tiny = LstmTagger(glove_embeddings_tiny, lstm, vocab_tiny)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3h_UPgIc-H3",
        "colab_type": "text"
      },
      "source": [
        "Train the basic model for the tiny dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGoZ95H5c-H3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "basic_trainer_tiny=initialize_trainer(model_tiny,vocab_tiny,train_dataset_tiny,validation_dataset_tiny,batch_size=50)\n",
        "basic_trainer_tiny.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1cUIzrLc-H6",
        "colab_type": "text"
      },
      "source": [
        "You have trained an LSTM POS tagger for the basic model. Now train the <code>glove_model_tiny</code>. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctZq3HHzc-H6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Write your code here\n",
        "glove_trainer_tiny=initialize_trainer(glove_model_tiny,vocab_tiny,train_dataset_tiny,validation_dataset_tiny,batch_size=50)\n",
        "glove_trainer_tiny.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOCfOBWsc-H9",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 3: Explore training parameters (10 points)\n",
        "\n",
        "Create separate models on the basis of bigger datasets: the 500 sentence training and 500 sentence validation and 5000 sentence training and 5000 sentence validation. Using the full training set (50K sentences) is optional (your machine might be too slow). Initialize and train the basic model on 500 sentence training and 500 sentence validation data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJg2U6kVc-H9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#train the basic model on 500 sentences\n",
        "\n",
        "train_dataset_tiny_500 = reader.read(\"train_brown_500.txt\")\n",
        "validation_dataset_tiny_500 = reader.read(\"validation_brown_500.txt\")\n",
        "vocab_tiny_500 = Vocabulary.from_instances(train_dataset_tiny_500 + validation_dataset_tiny_500)\n",
        "\n",
        "token_embedding_tiny_500 = Embedding(num_embeddings=vocab_tiny_500.get_vocab_size('tokens'),\n",
        "                            embedding_dim=EMBEDDING_DIM)\n",
        "\n",
        "word_embeddings_tiny_500 = BasicTextFieldEmbedder({\"tokens\": token_embedding_tiny_500})\n",
        "model_tiny_500 = LstmTagger(word_embeddings_tiny_500, lstm, vocab_tiny_500)\n",
        "\n",
        "basic_trainer_tiny_500=initialize_trainer(model_tiny_500,vocab_tiny_500,train_dataset_tiny_500,validation_dataset_tiny_500,batch_size=5)\n",
        "basic_trainer_tiny_500.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJc9nSRZc-IA",
        "colab_type": "text"
      },
      "source": [
        "Now do the same training (500 sentence training and 500 sentence validation sets) with GloVE embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMveroXQc-IA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_token_embedding_tiny_500 = Embedding.from_params(vocab=vocab_tiny_500,\n",
        "                            params=Params({'pretrained_file':'glove.6B.300d.txt',\n",
        "                                           'embedding_dim' : EMBEDDING_DIM}))\n",
        "glove_word_embeddings_tiny_500 = BasicTextFieldEmbedder({\"tokens\": glove_token_embedding_tiny_500})\n",
        "glove_model_tiny_500 = LstmTagger(glove_word_embeddings_tiny_500, lstm, vocab_tiny_500)\n",
        "\n",
        "glove_trainer_tiny_500=initialize_trainer(glove_model_tiny_500,vocab_tiny_500,train_dataset_tiny_500,validation_dataset_tiny_500,batch_size=50)\n",
        "glove_trainer_tiny_500.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDLqWa1dc-ID",
        "colab_type": "text"
      },
      "source": [
        "Use a bigger training set now with 5K sentence training and 5K sentence validation sets and random initial embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XPY9twGc-IE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_tiny_5000 = reader.read(\"train_brown_5000.txt\")\n",
        "validation_dataset_tiny_5000 = reader.read(\"validation_brown_5000.txt\")\n",
        "vocab_tiny_5000 = Vocabulary.from_instances(train_dataset_tiny_5000 + validation_dataset_tiny_5000)\n",
        "\n",
        "token_embedding_tiny_5000 = Embedding(num_embeddings=vocab_tiny_5000.get_vocab_size('tokens'),\n",
        "                            embedding_dim=EMBEDDING_DIM)\n",
        "\n",
        "word_embeddings_tiny_5000 = BasicTextFieldEmbedder({\"tokens\": token_embedding_tiny_5000})\n",
        "model_tiny_5000 = LstmTagger(word_embeddings_tiny_5000, lstm, vocab_tiny_5000)\n",
        "\n",
        "basic_trainer_tiny_5000=initialize_trainer(model_tiny_5000,vocab_tiny_5000,train_dataset_tiny_5000,validation_dataset_tiny_5000,batch_size=50)\n",
        "basic_trainer_tiny_5000.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQmoGLOnc-IG",
        "colab_type": "text"
      },
      "source": [
        "Now do the same training (5K sentence training and 5K sentence validation sets) with GloVE embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0SSNRAWc-IH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_token_embedding_tiny_5000 = Embedding.from_params(vocab=vocab_tiny_5000,\n",
        "                            params=Params({'pretrained_file':'glove.6B.300d.txt',\n",
        "                                           'embedding_dim' : EMBEDDING_DIM}))\n",
        "glove_word_embeddings_tiny_5000 = BasicTextFieldEmbedder({\"tokens\": glove_token_embedding_tiny_5000})\n",
        "glove_model_tiny_5000 = LstmTagger(glove_word_embeddings_tiny_5000, lstm, vocab_tiny_5000)\n",
        "\n",
        "glove_trainer_tiny_5000=initialize_trainer(glove_model_tiny_5000,vocab_tiny_5000,train_dataset_tiny_5000,validation_dataset_tiny_5000,batch_size=50)\n",
        "glove_trainer_tiny_5000.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2VGrJ_Ec-IJ",
        "colab_type": "text"
      },
      "source": [
        "For each trained model, record validation accuracy and training duration (they are returned along with other training stats after training a model) and accuracy on the training set. Fill in the numbers in the table below:\n",
        "\n",
        "| model | validation accuracy | training accuracy | training duration|\n",
        "|-------|---------------------|---------------|-------------------------------------------\n",
        "| basic model on 50 sentences||||\n",
        "| glove model on 50 sentences||||\n",
        "| basic model on 500 sentences|0.73|0.93|0:04:12|\n",
        "| glove model on 500 sentences|0.78|0.91|0:05:32|\n",
        "| basic model on 5000 sentences|0.98|0.98|0:41:50|\n",
        "| glove model on 5000 sentences|0:95|0:95|0:41:54|\n",
        "\n",
        "**Question.** What do you conclude from these comparisons? when can it be especially beneficial to initialize a model with pretrained embeddings?\n",
        "\n",
        "**Answer.** WRITE YOU ANSWER HERE\n",
        "\n",
        "From the table, we see that the glove model generalizes better to the  validation data than the basic model. However, we also notiice that the perfomance of the basic model increased with larger training data. It is beneficially to use pre-trained data when you have small training set and some similarity between the pretrained model and actual training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Trq1Z73c-IJ",
        "colab_type": "text"
      },
      "source": [
        "During training, data is processed in batches so that the model performs computation for multiple examples simultaneously. How does batching affect model training? Modify the training to have smaller batches of data - let's use batches of 5 or 500 instead of 50. How does this affect the results? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TC7Kf_yc-IK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define your trainers with alternative batching here: batches of 5, 50 sentences\n",
        "basic_trainer_tiny_50_b5=initialize_trainer(model_tiny,vocab_tiny,train_dataset_tiny,validation_dataset_tiny,batch_size=5)\n",
        "basic_trainer_tiny_50_b5.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bugl_f4Oa3yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batches of 5, 500 sentences\n",
        "basic_trainer_tiny_500_b5=initialize_trainer(model_tiny_500,vocab_tiny_500,train_dataset_tiny_500,validation_dataset_tiny_500,batch_size=5)\n",
        "basic_trainer_tiny_500_b5.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bitMfbXac-IN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#batches of 500, 50 sentences\n",
        "basic_trainer_tiny_50_b500=initialize_trainer(model_tiny,vocab_tiny,train_dataset_tiny,validation_dataset_tiny,batch_size=500)\n",
        "basic_trainer_tiny_50_b500.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHwIFs0Ec-IQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#batches of 500, 500 sentences\n",
        "basic_trainer_tiny_500_b500=initialize_trainer(model_tiny_500,vocab_tiny_500,train_dataset_tiny_500,validation_dataset_tiny_500,batch_size=500)\n",
        "basic_trainer_tiny_500_b500.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2Pv0C4A-2r5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0e93904c-e8f3-4217-b238-ff3eb282801a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DwhNMxAc-IS",
        "colab_type": "text"
      },
      "source": [
        "Report your results below:\n",
        "\n",
        "**batches of 5**:\n",
        "\n",
        "| model | validation accuracy | training accuracy | training duration|\n",
        "|-------|---------------------|---------------|-------------------------------------------\n",
        "| basic model on 50 sentences|0.54|0.90|0:00:59|\n",
        "| basic model on 500 sentences|0.73|0.94|0:01:25|\n",
        "\n",
        "**batches of 500**:\n",
        "\n",
        "| model | validation accuracy | training accuracy | training duration|\n",
        "|-------|---------------------|---------------|-------------------------------------------\n",
        "| basic model on 50 sentences|0.55|0.92|0:00:37|\n",
        "| basic model on 500 sentences|0:73|0.95|0:02:12|\n",
        "\n",
        "**Question.** What do these results tell you?\n",
        "**Answer.** WRITE YOUR ANSWER HERE\n",
        "\n",
        "The larger the batch size the better the performance on both the training and validation set. By seeing more of the data during each training iteration, we see that the performance of model increases, however, we notice that the model significantly overfits when it has a small training data and a large batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O40_iF2tc-IS",
        "colab_type": "text"
      },
      "source": [
        "## Comment \n",
        "In this lab we used pretrained GloVe embeddings in a model for part of speech tagging. GloVe in its turn is also a neural word embedding model, but it had been trained on a completely different objective. GloVe vectors had been optimised on word cooccurrence matrix decomposition, i.e. on the task of predicting which words tend to occur with which other words. Part of speech certainly plays a role in determining statistical cooccurrence of words, but this role is indirect, and explicit part of speech information has not been used in training GloVe.\n",
        "\n",
        "This makes our application an example of **transfer learning**, whereby a learned model trained on one objective (e.g. word cooccurrence) can benefit a different application (e.g. POS tagging), because some information is shared between them. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01ApX-Pxc-IU",
        "colab_type": "text"
      },
      "source": [
        "## Part 2 - ELMo vectors (50 points)\n",
        "\n",
        "In the second part of this lab we will reproduce the word sense disambiguation strategy that the authors of the ELMo vectors explored. The strategy consists in the following:\n",
        "\n",
        "- create ELMo embeddings for all tokens in a sense-annotated corpus\n",
        "- calculate mean sense vectors for each word sense in the training partition of the corpus\n",
        "- for each sense-annotated token in the test partition of the corpus, assign it to the sense of the word to which its ELMo vector is the closest according to the cosine distance metric\n",
        "- as a backup strategy, use the 1st sense of the word by default.\n",
        "\n",
        "As a sense annotated corpus, we can use SemCor, conveniently available within NLTK. <code>semcor.sents()</code> iterates over all sentences represented as lists of tokens, while <code>semcor.tagged_sents()</code> iterates over the same sentences with additional annotation including WordNet lemma identifiers (lemmas in WordNet stand for a word taken in a specific sense)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EERhcBRqc-IU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "85daba68-c988-4693-fe21-92db353cfd2c"
      },
      "source": [
        "from nltk.corpus import semcor\n",
        "from nltk.corpus import wordnet as wn\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "#wn.lemmas(\"fish\")\n",
        "semcor.sents()\n",
        "semcor.tagged_sents(tag=\"sem\")\n",
        "semcor_sentences = list(semcor.tagged_sents(tag=\"sem\"))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqouB8pxc-IW",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1. Extract relevant data from SemCor (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qgMtBc9c-IX",
        "colab_type": "text"
      },
      "source": [
        "First, split all the sentences in SemCor randomly into 90% training and 10% testing partitions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSBtxN5-c-IX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#write your code here\n",
        "random.shuffle(semcor_sentences)\n",
        "semcor_train= semcor_sentences[:int(0.9*len(semcor_sentences))]\n",
        "semcor_test= semcor_sentences[int(0.9*len(semcor_sentences)):]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkcNN78lc-IZ",
        "colab_type": "text"
      },
      "source": [
        "Create a function that takes as input a sentence from SemCor and extracts a list which contains, for each token of the sentence, either the corresponding WordNet Lemma (e.g. <code>Lemma('friday.n.01.Friday')</code>) or <code>None</code>. <code>None</code> corresponds to tokens that are either 1) not annotated for word senses (e.g. articles); 2) are marked up as (part of) a named entity (e.g. \"City of Atlanta\" or placename \"Fulton\" annotated as  <code>Tree(Lemma('location.n.01.location'), [Tree('NE', ['Fulton'])])</code>)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfob3Dycc-IZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_lemmas(semcor_sentence):\n",
        "  #sentence= semcor_sentence\n",
        "  word_to_lemma = dict()\n",
        "  for i in range(len(semcor_sentence)):\n",
        "    if str(type(semcor_sentence[i])) == \"<class 'nltk.tree.Tree'>\":\n",
        "      temp = str(semcor_sentence[i])\n",
        "      temp = temp.strip('( )')\n",
        "      if len(temp.split()) == 2 and  (\"Lemma('location.n.01.location')\" not in temp or \"(NE\" not in temp):\n",
        "        word_to_lemma[temp.split()[1]] =  temp.split()[0]\n",
        "      elif len(temp.split()) > 2 or (\"Lemma('location.n.01.location')\" in temp or \"(NE\"  in temp):\n",
        "        for token in temp.split()[1:]:\n",
        "          if token != \"(NE\":\n",
        "            word_to_lemma[token] = \"None\"\n",
        "    elif type(semcor_sentence[i]) == list:\n",
        "      word_to_lemma[\"\".join(semcor_sentence[i])] = \"None\"\n",
        "\n",
        "\n",
        "  return word_to_lemma    #we return a dictionary instead of just the list of lemmas/none incase we need to do a lookup later and always get the list of values from the dict\n",
        " "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuVRU3goc-Ie",
        "colab_type": "text"
      },
      "source": [
        "You are now able to extract word senses (instantiated by WordNet lemmas) from the corpus. The next step is to associate senses with ELMo vectors. Create a dictionary of contextualized token embeddings from the training corpus grouped by the WordNet sense:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBRtPhUvc-If",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import numpy\n",
        "\n",
        "Train_embeddings=defaultdict(list)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbA0CqN3c-Ih",
        "colab_type": "text"
      },
      "source": [
        "Now let's create contextualized ELMo word embeddings for the tokens in this corpus. We can load the pretrained ELMo model and define a function <code>sentences_to_elmo()</code> that receives a list of tokenized sentences as input and produces their ELMo vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6gCcOauc-Ih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
        "\n",
        "options_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
        "weight_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
        "elmo = Elmo(options_file, weight_file, 1, dropout=0)\n",
        "\n",
        "def sentences_to_elmo(sentences):\n",
        "    character_ids = batch_to_ids(sentences)\n",
        "    embeddings = elmo(character_ids)\n",
        "    return embeddings"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcDraZYDc-Ij",
        "colab_type": "text"
      },
      "source": [
        "Now you can process the corpus sentences and produce their ELMo vectors. It is recommended to pass the input to ELMo encoder in batches. A suggested batch size is 50 sentences. For example, the code below processes the first 50 sentences from the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCZlKZiyc-Ij",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "52b0b11a-48e0-4688-9e02-7634eedabb32"
      },
      "source": [
        "sentences=semcor.sents()[:50]\n",
        "embeddings=sentences_to_elmo(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', \"'s\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term', 'end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ['The', 'September', 'October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.'], ['``', 'Only', 'a', 'relative', 'handful', 'of', 'such', 'reports', 'was', 'received', \"''\", ',', 'the', 'jury', 'said', ',', '``', 'considering', 'the', 'widespread', 'interest', 'in', 'the', 'election', ',', 'the', 'number', 'of', 'voters', 'and', 'the', 'size', 'of', 'this', 'city', \"''\", '.'], ['The', 'jury', 'said', 'it', 'did', 'find', 'that', 'many', 'of', 'Georgia', \"'s\", 'registration', 'and', 'election', 'laws', '``', 'are', 'outmoded', 'or', 'inadequate', 'and', 'often', 'ambiguous', \"''\", '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0Dw6iCUc-Il",
        "colab_type": "text"
      },
      "source": [
        "The <code>embeddings</code> that we obtained is a dictionary that contains a list of ELMo embeddings and a list of masks. The mask tells us which embeddings correspond to tokens in the original input sentences and which correspond to the padding (introduced to give all sentences in the batch the same length).\n",
        "In principle all embeddings are stored in PyTorch tensors so that they can be used in bigger neural models, but we are not going to do it now. For our purposes, PyTorch tensors can be converted to numpy arrays:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qu6iaiec-Im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings['elmo_representations'][0].detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBtjCw_ac-Io",
        "colab_type": "text"
      },
      "source": [
        "We can check the size of the embeddings we got. It has three dimensions: 1) the number of sentences 2) the number of tokens (corresponds to the tokens in the longest original sentence of the batch; shorter ones were padded)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6YaDw4Oc-Io",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e73d481d-f64f-4232-ba20-35eaedb242e2"
      },
      "source": [
        "embeddings['elmo_representations'][0].detach().size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 59, 1024])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_xIjCXoc-Iq",
        "colab_type": "text"
      },
      "source": [
        "Another thing contained in the <code>embeddings</code> is the mask, a tensor encoding which tokens vectors correspond to original tokens and which are paddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDFaFHPfc-Ir",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "235b7af7-c880-4fb2-bc73-cdaf9fc32bb6"
      },
      "source": [
        "embeddings['mask']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UztI4wuYc-Iu",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2. Extract ELMo encoding of sentences using a mask (5 points)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaCuImhZc-Iv",
        "colab_type": "text"
      },
      "source": [
        "Now define a function <code>get_masked_vectors(embeddings)</code> that takes embeddings as input and returns a list of ELMo sentence encodings to which the mask has been applied, i.e. where the padding vectors have been removed so the representation of each sentence contains as many vectors as there were tokens in the original sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSDlTvW_c-Iw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_masked_vectors(embeddings):\n",
        "    #Your code here\n",
        "    sentences =list()\n",
        "    emb = embeddings['elmo_representations'][0].detach().numpy()\n",
        "    for i in range(len(embeddings['mask'])):\n",
        "      index_no_padding = max(np.where(embeddings['mask'][i] == 1)[0])\n",
        "      sentence = emb[i,:index_no_padding+1]\n",
        "      sentences.append(sentence)\n",
        "\n",
        "    return sentences\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzjvxDNUc-Iz",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 3. Collect ELMo vectors from the training corpus (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riezKJesc-Iz",
        "colab_type": "text"
      },
      "source": [
        "Process the corpus updating your train word sense vectors. Iterate over the all the train sentences in the corpus, and retrieve for each lemma-annotated token (where lemma is not <code>None</code>) the corresponding ELMo vector. Store the ELMo sense embeddings that correspond to each lemma in the dictionary <code>Train_embeddings</code>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKU2d6_csjy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del Train_embeddings"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHkXR2J2c-I0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Your code here\n",
        "counter =0\n",
        "j_index=50\n",
        "i_index=0\n",
        "while j_index <= len(semcor_train[:10000]): #RAM issues when i try to train on more dataset \n",
        "  print(i_index,j_index)\n",
        "  token_with_lemmas = list()\n",
        "  lemmas_list = list()\n",
        "  tokens_ = list()\n",
        "  lemmas_ = list()\n",
        "  semcor_train_ = semcor_train[i_index : j_index]\n",
        "\n",
        "  for each in semcor_train_:\n",
        "    lemas = get_lemmas(each)\n",
        "    for tokken, lemma in lemas.items():\n",
        "      if lemma != \"None\":\n",
        "        token_with_lemmas.append(tokken)\n",
        "        lemmas_list.append(lemma)\n",
        "    if len(token_with_lemmas) !=0:\n",
        "      tokens_.append(token_with_lemmas)\n",
        "      lemmas_.append(lemmas_list)\n",
        "    token_with_lemmas = []\n",
        "    lemmas_list = []\n",
        "\n",
        "  sentence_encode = get_masked_vectors(sentences_to_elmo(tokens_))\n",
        "  for i in range(len(sentence_encode)):\n",
        "    for j, lemma in enumerate(lemmas_[i]):\n",
        "      Train_embeddings[lemma].append(sentence_encode[i][j]) \n",
        "  tokens_ =[]\n",
        "  lemmas_ = []\n",
        "  sentence_encode = []\n",
        "  i_index = j_index\n",
        "  j_index += 50\n",
        "  \n",
        "\n",
        "#divide training embeddings into different dictionaries then combine at the end. \n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IJC0fPFc-I1",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 4. Vector averaging (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPSXe4ygc-I2",
        "colab_type": "text"
      },
      "source": [
        "Now you can calculate the average ELMo vector for each word sense in the training corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVgm97_Pc-I3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Your code here\n",
        "Train_embeddings_=dict()\n",
        "sum_vec =0\n",
        "for lemma, vectors  in Train_embeddings.items():\n",
        "  length_vec = len(vectors)\n",
        "  for vector in vectors:\n",
        "    sum_vec += vector/ length_vec\n",
        "  Train_embeddings_[lemma] = sum_vec"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k7rUShxc-I5",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 5. Testing the sense vectors (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7DvVLvDc-I5",
        "colab_type": "text"
      },
      "source": [
        "Test your sense embeddings on your test data, which is a subset of the SemCor corpus. Use the strategy outlined above, with 1st WordNet sense as a fallback: \n",
        "\n",
        "- rely on mean sense vectors for each word sense in the training partition of the corpus\n",
        "- for each sense-annotated token <i>t</i> (e.g. the verb \"run\") in the test partition of the corpus, assign it to the sense of the word \"Lemma(*.v*.run)\" to which ithe ELMo vector <i>t</i> is the closest according to the cosine distance metric\n",
        "- as a backup strategy, use the 1st sense of the word (e.g. <code>Lemma(run.v.01.run)</code>) by default.\n",
        "\n",
        "Report WSD accuracy in percentage points on your test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTNrkGHZc-I6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "lema_vec = list()\n",
        "lemmas_sysnet =list()\n",
        "backup_strategy = list()\n",
        "cosine_values = list()\n",
        "predicted_lemma = defaultdict(list)\n",
        "lemmas_in_training = list()\n",
        "count = 0\n",
        "total_lemmas_count =0\n",
        "for sentences in semcor_test:\n",
        "  sentence_dict = get_lemmas(sentences)\n",
        "  for word, values in sentence_dict.items():\n",
        "    if values != \"None\":\n",
        "      total_lemmas_count +=1\n",
        "      lemmas = wn.lemmas(word)\n",
        "      lemmas_sysnet.append(wn.synsets(word))\n",
        "      lemmas_sysnet_ =[item for i in lemmas_sysnet for item in i]\n",
        "      for i,lemma in enumerate(lemmas):\n",
        "        if str(lemma) in Train_embeddings_.keys():\n",
        "          lemma_elmo = get_masked_vectors(sentences_to_elmo(lemmas_sysnet_[i].name()))[0][0]\n",
        "          elmo_cosine_score = cosine_similarity(Train_embeddings_[str(lemma)].reshape((1,-1)), lemma_elmo.reshape(1,-1))\n",
        "          cosine_values.append(elmo_cosine_score)\n",
        "          lemmas_in_training.append(lemma)\n",
        "        else:\n",
        "          backup = lemmas[0]\n",
        "          backup_strategy.append(backup)\n",
        " \n",
        "      cosine_values_ = sorted(cosine_values, reverse= True)\n",
        "      lemmas_in_training_ = [x for _,x in sorted(zip(cosine_values,lemmas_in_training),reverse=True)]\n",
        "\n",
        "      if len(lemmas_in_training_) != 0:\n",
        "        if str(values) == str(lemmas_in_training_[0]):\n",
        "          count += 1\n",
        "      elif len(backup_strategy) !=0:\n",
        "        if str(values) == str(backup_strategy[0]):\n",
        "          count += 1\n",
        "\n",
        "      lema_vec = []\n",
        "      lemmas_sysnet =[]\n",
        "      backup_strategy = []\n",
        "      cosine_values = []\n",
        "      lemmas_in_training_ =[]\n",
        "\n",
        "\n",
        "#Accuracy\n",
        "\n",
        "accuracy =  count/total_lemmas_count\n",
        "\n",
        "\n",
        "\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IohQF69hc-I8",
        "colab_type": "text"
      },
      "source": [
        "## The end\n",
        "Congratulations! this is the end of Lab 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVEJFAmec-I8",
        "colab_type": "text"
      },
      "source": [
        "**Acknowledgements** Tejaswini Deoskar has given valuable comments that helped improve this lab assignment. Timothee Mickus helped to test this assignment and gave extensive feedback on the instructions. Many thanks to both."
      ]
    }
  ]
}